{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10bf3438",
      "metadata": {
        "id": "10bf3438"
      },
      "source": [
        "# Ассистент 1 - LM на основе n-грамм"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de848df",
      "metadata": {
        "id": "6de848df"
      },
      "source": [
        "Перед вами - первое дополнительное задание повышенной сложности, в рамках которого вам предстоит начать разработку телеграм-бота с генеративными моделями.\n",
        "\n",
        "Цель данного ноутбука - помочь влиться в разработку ассистента. В данном ноутбуке написан код для \"обучения\" LM на основе n-грамм, для генерации с помощью нее текста, а также сохранение и загрузка модели и токенизатора.\n",
        "\n",
        "Относитесь к данному заданию максимально творчески - любую часть кода можно менять под ваши нужды и желания, можно оптимизировать, добавлять методы генерации, использовать любые данные, обучать сколь угодно \"большую\" модель.\n",
        "\n",
        "При этом вам стоит быть готовыми со всеми техническими проблеми справляться самому - именно так обычно происходит в реальной жизни в реальных проектах :)\n",
        "\n",
        "Поэтому отдельно подчеркну:\n",
        "* если что-то сломалось после ваших изменений - подразумевается, что вы сами найдете проблему и исправите\n",
        "* если вы ничего не трогали, но что-то не работает у нас - подразумевается, что вы сами найдете проблему и исправите :)\n",
        "\n",
        "Главный критерий выполнености данного задания - телеграм-бот, генерирующий текст и использующий обозначенный в задании подход (в случае данного ноутбука - n-граммная модель в любой ее реализации)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aecb8bd4",
      "metadata": {
        "id": "aecb8bd4"
      },
      "source": [
        "_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ca60e8",
      "metadata": {
        "id": "31ca60e8"
      },
      "source": [
        "Для обучения качественной модели вам потребуются датасеты. В ноутбуке составлен маленький игрушечный датасет, вам для улучшения качества потребуется данные в большем количестве и более качественные, а также другие параметры модели и генерации (например, размер контекста побольше).\n",
        "\n",
        "С нормальным датасетом и правильными параметрами даже такой простой моделью можно добиться адекватного качества генерации текста (возможно не очень человечный, но вполне связный текст).\n",
        "\n",
        "Датасеты можно найти и выбрать тут (желательно на русском, вам так будет понятней качество и в целом полезней):\n",
        "https://huggingface.co/datasets\n",
        "  \n",
        "Можете найти наиболее интересный для себя датасет (можете сделать модель как смешной, так и полезной), либо выбрать любой из этих датасетов\n",
        "* https://huggingface.co/datasets/Den4ikAI/russian_dialogues\n",
        "* https://huggingface.co/datasets/Georgii/russianPoetry\n",
        "* https://huggingface.co/datasets/IgorVolochay/russian_jokes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0c6c33a9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T14:50:17.414585Z",
          "iopub.status.busy": "2024-02-21T14:50:17.413838Z",
          "iopub.status.idle": "2024-02-21T14:50:37.187697Z",
          "shell.execute_reply": "2024-02-21T14:50:37.186316Z",
          "shell.execute_reply.started": "2024-02-21T14:50:17.414553Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c6c33a9",
        "outputId": "50d49c3a-9d00-4f88-9da0-94cbe2cf7d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pickle\n",
        "from itertools import chain\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "from typing import List, Dict, Optional, Iterable, Tuple\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "! pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.processors import TemplateProcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df809daa",
      "metadata": {
        "id": "df809daa"
      },
      "source": [
        "Токенизатор разбивает текст на слова. Можно попробовать другие способы токенизации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "90160389",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:11.789756Z",
          "iopub.status.busy": "2024-02-21T15:03:11.788899Z",
          "iopub.status.idle": "2024-02-21T15:03:11.808584Z",
          "shell.execute_reply": "2024-02-21T15:03:11.807519Z",
          "shell.execute_reply.started": "2024-02-21T15:03:11.789725Z"
        },
        "id": "90160389"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self,\n",
        "                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:]',\n",
        "                 eos_token: str = '<EOS>',\n",
        "                 pad_token: str = '<PAD>',\n",
        "                 unk_token: str = '<UNK>'):\n",
        "        self.token_pattern = token_pattern\n",
        "        self.eos_token = eos_token\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "\n",
        "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
        "        self.vocab = None\n",
        "        self.inverse_vocab = None\n",
        "\n",
        "    def text_preprocess(self, input_text: str) -> str:\n",
        "        \"\"\" Предобрабатываем один текст \"\"\"\n",
        "        # input_text = ... # приведение к нижнему регистру\n",
        "        input_text = input_text.lower()\n",
        "        input_text = re.sub('\\s+', ' ', input_text) # унифицируем пробелы\n",
        "        input_text = input_text.strip()\n",
        "        return input_text\n",
        "\n",
        "    def build_vocab(self, corpus: List[str]) -> None:\n",
        "        assert len(corpus)\n",
        "        all_tokens = set()\n",
        "        for text in corpus:\n",
        "            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n",
        "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
        "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
        "        for token in special_tokens:\n",
        "            self.vocab[token] = len(self.vocab)\n",
        "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
        "        return self\n",
        "\n",
        "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
        "        text = self.text_preprocess(text)\n",
        "        tokens = re.findall(self.token_pattern, text)\n",
        "        if append_eos_token:\n",
        "            tokens.append(self.eos_token)\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
        "        \"\"\" Токенизируем текст \"\"\"\n",
        "        tokens = self._tokenize(text, append_eos_token)\n",
        "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
        "        assert len(input_ids)\n",
        "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
        "        tokens = []\n",
        "        for ind in input_ids:\n",
        "            token = self.inverse_vocab[ind]\n",
        "            if remove_special_tokens and token in self.special_tokens:\n",
        "                continue\n",
        "            tokens.append(token)\n",
        "        text = ' '.join( tokens )\n",
        "        return text\n",
        "\n",
        "    def save(self, path: str) -> bool:\n",
        "        data = {\n",
        "            'token_pattern': self.token_pattern,\n",
        "            'eos_token': self.eos_token,\n",
        "            'pad_token': self.pad_token,\n",
        "            'unk_token': self.unk_token,\n",
        "            'special_tokens': self.special_tokens,\n",
        "            'vocab': self.vocab,\n",
        "            'inverse_vocab': self.inverse_vocab,\n",
        "        }\n",
        "\n",
        "        with open(path, 'wb') as fout:\n",
        "            pickle.dump(data, fout)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load(self, path: str) -> bool:\n",
        "        with open(path, 'rb') as fin:\n",
        "            data = pickle.load(fin)\n",
        "\n",
        "        self.token_pattern = data['token_pattern']\n",
        "        self.eos_token = data['eos_token']\n",
        "        self.pad_token = data['pad_token']\n",
        "        self.unk_token = data['unk_token']\n",
        "        self.special_tokens = data['special_tokens']\n",
        "        self.vocab = data['vocab']\n",
        "        self.inverse_vocab = data['inverse_vocab']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6999f620",
      "metadata": {
        "id": "6999f620"
      },
      "source": [
        "Класс для задания параметров генерации, так удобней писать логику для валидации параметров и разные другие доп методы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f14d758b",
      "metadata": {
        "id": "f14d758b"
      },
      "outputs": [],
      "source": [
        "class GenerationConfig:\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Тут можно задать любые параметры и их значения по умолчанию\n",
        "        Значения для стратегии декодирования decoding_strategy: ['max', 'top-p']\n",
        "        \"\"\"\n",
        "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
        "        self.max_tokens = kwargs.pop(\"max_tokens\", 32)\n",
        "        self.sample_top_p = kwargs.pop(\"sample_top_p\", 0.9)\n",
        "        self.decoding_strategy = kwargs.pop(\"decoding_strategy\", 'max')\n",
        "        self.remove_special_tokens = kwargs.pop(\"remove_special_tokens\", False)\n",
        "        self.validate()\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\" Здесь можно валидировать параметры \"\"\"\n",
        "        if not (1.0 > self.sample_top_p > 0):\n",
        "            raise ValueError('sample_top_p')\n",
        "        if self.decoding_strategy not in ['max', 'top-p']:\n",
        "            raise ValueError('decoding_strategy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd923a4",
      "metadata": {
        "id": "6bd923a4"
      },
      "source": [
        "Сама LM на основе n-грамм. Тут используется сглаживание Лапласа (можно поменять на метод backoff при желании), а также есть ряд параметров, сильно влияющий на качество генерации. Один из параметров генерации - стратегия генерации.\n",
        "\n",
        "Когда мы получили вероятности для следующего токена, мы по этим вероятностям хотим выбрать этот следующий токен.\n",
        "\n",
        "Можно просто семплировать из этого распределения - но тогда есть шанс, что будут очень маловероятные токены.\n",
        "\n",
        "Можно брать самый вероятный токен - но это плохо повлияет на разнообразие и \"человечность\" языка\n",
        "\n",
        "Можно воспользовать подходом top-p - семплировать только из тех токенов, которые наиболее вероятны (их вероятности суммируются в заданный p)\n",
        "\n",
        "Можно проверить, что top-p будет генерировать более интересный текст чем max\n",
        "\n",
        "Также обратите внимание на параметр температуры. В случае top-p и семплирования, чем больше делаешь температуру, тем меньше отличаются друг от друга вероятности (распределение стремится к равномерному, даже если исходное распределение имело вполне себе выраженные максимумы), и текст становится более случайным (и разнообразным)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "462316de",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:21.268282Z",
          "iopub.status.busy": "2024-02-21T15:03:21.267567Z",
          "iopub.status.idle": "2024-02-21T15:03:21.292501Z",
          "shell.execute_reply": "2024-02-21T15:03:21.291323Z",
          "shell.execute_reply.started": "2024-02-21T15:03:21.268251Z"
        },
        "id": "462316de"
      },
      "outputs": [],
      "source": [
        "class StatLM:\n",
        "    def __init__(self,\n",
        "                 tokenizer: Tokenizer,\n",
        "                 context_size: int = 2,\n",
        "                 alpha: float = 0.1\n",
        "                ):\n",
        "\n",
        "        assert context_size >= 2\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.n_gramms_stat = defaultdict(int)\n",
        "        self.nx_gramms_stat = defaultdict(int)\n",
        "\n",
        "    def get_token_by_ind(ind: int) -> str:\n",
        "        return self.tokenizer.vocab.get(ind)\n",
        "\n",
        "    def get_ind_by_token(token: str) -> int:\n",
        "        return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.unk_token])\n",
        "\n",
        "    def train(self, train_texts: List[str]):\n",
        "        for sentence in tqdm(train_texts, desc='train lines'):\n",
        "            sentence_ind = self.tokenizer.encode(sentence)\n",
        "            for i in range(len(sentence_ind) - self.context_size):\n",
        "\n",
        "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
        "                self.n_gramms_stat[seq] += 1\n",
        "\n",
        "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
        "                self.nx_gramms_stat[seq_x] += 1\n",
        "\n",
        "            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
        "            self.n_gramms_stat[seq] += 1\n",
        "\n",
        "    def sample_token(self,\n",
        "                     token_distribution: np.ndarray,\n",
        "                     generation_config: GenerationConfig) -> int:\n",
        "        if generation_config.decoding_strategy == 'max':\n",
        "            return token_distribution.argmax()\n",
        "        elif generation_config.decoding_strategy == 'top-p':\n",
        "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
        "                                        reverse=True)\n",
        "            total_proba = 0.0\n",
        "            tokens_to_sample = []\n",
        "            tokens_probas = []\n",
        "            for token_proba, ind in token_distribution:\n",
        "                tokens_to_sample.append(ind)\n",
        "                tokens_probas.append(token_proba)\n",
        "                total_proba += token_proba\n",
        "                if total_proba >= generation_config.sample_top_p:\n",
        "                    break\n",
        "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
        "            tokens_probas = np.array(tokens_probas) / generation_config.temperature\n",
        "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
        "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
        "        else:\n",
        "            raise ValueError(f'Unknown decoding strategy: {generation_config.decoding_strategy}')\n",
        "\n",
        "    def save_stat(self, path: str) -> bool:\n",
        "        stat = {\n",
        "            'n_gramms_stat': self.n_gramms_stat,\n",
        "            'nx_gramms_stat': self.nx_gramms_stat,\n",
        "            'context_size': self.context_size,\n",
        "            'alpha': self.alpha\n",
        "        }\n",
        "        with open(path, 'wb') as fout:\n",
        "            pickle.dump(stat, fout)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def load_stat(self, path: str) -> bool:\n",
        "        with open(path, 'rb') as fin:\n",
        "            stat = pickle.load(fin)\n",
        "\n",
        "        self.n_gramms_stat = stat['n_gramms_stat']\n",
        "        self.nx_gramms_stat = stat['nx_gramms_stat']\n",
        "        self.context_size = stat['context_size']\n",
        "        self.alpha = stat['alpha']\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_stat(self) -> Dict[str, Dict]:\n",
        "\n",
        "        n_token_stat, nx_token_stat = {}, {}\n",
        "        for token_inds, count in self.n_gramms_stat.items():\n",
        "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
        "\n",
        "        for token_inds, count in self.nx_gramms_stat.items():\n",
        "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
        "\n",
        "        return {\n",
        "            'n gramms stat': self.n_gramms_stat,\n",
        "            'n+1 gramms stat': self.nx_gramms_stat,\n",
        "            'n tokens stat': n_token_stat,\n",
        "            'n+1 tokens stat': nx_token_stat,\n",
        "        }\n",
        "\n",
        "    def _get_next_token(self,\n",
        "                        tokens: List[int],\n",
        "                        generation_config: GenerationConfig) -> (int, str):\n",
        "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + self.alpha * len(self.tokenizer.vocab)\n",
        "        numerators = []\n",
        "        for ind in self.tokenizer.inverse_vocab:\n",
        "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + self.alpha)\n",
        "\n",
        "        token_distribution = np.array(numerators) / denominator\n",
        "        max_proba_ind = self.sample_token(token_distribution, generation_config)\n",
        "\n",
        "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
        "\n",
        "        return max_proba_ind, next_token\n",
        "\n",
        "    def generate_token(self,\n",
        "                       text: str,\n",
        "                       generation_config: GenerationConfig\n",
        "                      ) -> Dict:\n",
        "        tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
        "        tokens = tokens[-self.context_size + 1:]\n",
        "\n",
        "        max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
        "\n",
        "        return {\n",
        "            'next_token': next_token,\n",
        "            'next_token_num': max_proba_ind,\n",
        "        }\n",
        "\n",
        "\n",
        "    def generate_text(self, text: str,\n",
        "                      generation_config: GenerationConfig\n",
        "                     ) -> Dict:\n",
        "\n",
        "        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
        "        tokens = all_tokens[-self.context_size + 1:]\n",
        "\n",
        "        next_token = None\n",
        "        while next_token != self.tokenizer.eos_token and len(all_tokens) < generation_config.max_tokens:\n",
        "            max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
        "            all_tokens.append(max_proba_ind)\n",
        "            tokens = all_tokens[-self.context_size + 1:]\n",
        "\n",
        "        new_text = self.tokenizer.decode(all_tokens, generation_config.remove_special_tokens)\n",
        "\n",
        "        finish_reason = 'max tokens'\n",
        "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
        "            finish_reason = 'end of text'\n",
        "\n",
        "        return {\n",
        "            'all_tokens': all_tokens,\n",
        "            'total_text': new_text,\n",
        "            'finish_reason': finish_reason\n",
        "        }\n",
        "\n",
        "    def generate(self, text: str, generation_config: Dict) -> str:\n",
        "        return self.generate_text(text, generation_config)['total_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f60f19",
      "metadata": {
        "id": "90f60f19"
      },
      "source": [
        "Эта функция напрямую используется в телеграм боте для получения модели и конфига генерации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4cfdad05",
      "metadata": {
        "id": "4cfdad05"
      },
      "outputs": [],
      "source": [
        "def construct_model():\n",
        "    config = {\n",
        "        'temperature': 1.0,\n",
        "        'max_tokens': 32,\n",
        "        'sample_top_p': 0.9,\n",
        "        'decoding_strategy': 'top-p',\n",
        "    }\n",
        "\n",
        "    stat_lm_path = 'models/stat_lm/stat_lm.pkl'\n",
        "    tokenizer_path = 'models/stat_lm/tokenizer.pkl'\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.load(tokenizer_path)\n",
        "\n",
        "    stat_lm = StatLM(tokenizer)\n",
        "    stat_lm.load_stat(stat_lm_path)\n",
        "\n",
        "    generation_config = GenerationConfig(temperature=config['temperature'],\n",
        "                                         max_tokens=config['max_tokens'],\n",
        "                                         sample_top_p=config['sample_top_p'],\n",
        "                                         decoding_strategy=config['decoding_strategy'],\n",
        "                                         remove_special_tokens=True)\n",
        "\n",
        "    kwargs = {'generation_config': generation_config}\n",
        "    return stat_lm, kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a85b767",
      "metadata": {
        "id": "3a85b767"
      },
      "source": [
        "### Обучаем на игрушечных данных"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2674d63a",
      "metadata": {
        "id": "2674d63a"
      },
      "source": [
        "Для демонстрации того, что происходит, возьмем несколько коротких цитат Джейсона Стэтхема отсюда:\n",
        "\n",
        "https://dzen.ru/a/ZRFaGN_gKhX6xTWW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6b515d5a",
      "metadata": {
        "id": "6b515d5a"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(temperature = 1.0, max_tokens = 32,\n",
        "                                     sample_top_p = 0.9, decoding_strategy = 'max',\n",
        "                                     remove_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "texts = pd.read_csv('dataset.csv', encoding='latin1')"
      ],
      "metadata": {
        "id": "l5xn9GtqYC7K"
      },
      "id": "l5xn9GtqYC7K",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7fa25b3b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:23.642385Z",
          "iopub.status.busy": "2024-02-21T15:03:23.641563Z",
          "iopub.status.idle": "2024-02-21T15:03:23.647640Z",
          "shell.execute_reply": "2024-02-21T15:03:23.646711Z",
          "shell.execute_reply.started": "2024-02-21T15:03:23.642338Z"
        },
        "id": "7fa25b3b"
      },
      "outputs": [],
      "source": [
        "train_texts = texts[:-1]\n",
        "test_text = texts[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "98e92a50",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:25.218697Z",
          "iopub.status.busy": "2024-02-21T15:03:25.218225Z",
          "iopub.status.idle": "2024-02-21T15:03:25.223271Z",
          "shell.execute_reply": "2024-02-21T15:03:25.222261Z",
          "shell.execute_reply.started": "2024-02-21T15:03:25.218665Z"
        },
        "id": "98e92a50"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer().build_vocab(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f61a4e82",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:25.398291Z",
          "iopub.status.busy": "2024-02-21T15:03:25.397981Z",
          "iopub.status.idle": "2024-02-21T15:03:25.406154Z",
          "shell.execute_reply": "2024-02-21T15:03:25.405134Z",
          "shell.execute_reply.started": "2024-02-21T15:03:25.398265Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f61a4e82",
        "outputId": "3b025115-16a2-481e-a6ab-d4d0ae447dd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 0, '<EOS>': 1, '<UNK>': 2, '<PAD>': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dict(list(tokenizer.vocab.items())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2ef0948d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T15:03:29.936948Z",
          "iopub.status.busy": "2024-02-21T15:03:29.936563Z",
          "iopub.status.idle": "2024-02-21T15:03:29.957657Z",
          "shell.execute_reply": "2024-02-21T15:03:29.956641Z",
          "shell.execute_reply.started": "2024-02-21T15:03:29.936919Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d51863cc05c647159cef8e934b4df19d",
            "a2dff33985044707874ea8bce01207d5",
            "66913ce89ee84cbe830052c6ce5d3d27",
            "77d60cf04d5248a8ba8adbb52d32e303",
            "31eba6d03bf742c7a554ba0f3c3396d4",
            "185880a338bb4accb424a4b612566271",
            "c27fe85b9d0344998143016ed0141db5",
            "2b6ede97630f4bd698d5b132085a04e7",
            "8d4737b8d7124fe79af7248703656021",
            "a5a36e574875457c818a6852208109cb",
            "ebb58cf9ef2b4ba396cf05d9d5a1e3b1"
          ]
        },
        "id": "2ef0948d",
        "outputId": "3cdc86d7-21af-4166-c9b4-6f28cfadbcd7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train lines:   0%|          | 0/150552 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d51863cc05c647159cef8e934b4df19d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# класс, который позволяем строить и использовать языковую модель на основе n-грамм\n",
        "stat_lm = StatLM(tokenizer, context_size=2, alpha=0.1) # , sample_top_p = None\n",
        "\n",
        "# \"обучаем\" модель - считаем статистики\n",
        "stat_lm.train(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1e73f538",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e73f538",
        "outputId": "59adbdf1-6781-4b74-f411-dcd89847b459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text text text text text text text text text text text text text text text text text text text text text text text text text text text text\n"
          ]
        }
      ],
      "source": [
        "print(stat_lm.generate(\"вот такие пироги!\", generation_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d054a1d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d054a1d4",
        "outputId": "990d3e52-fd17-4285-a315-44235d6ca92c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenizer.save('/content/tokenizer.pkl')\n",
        "stat_lm.save_stat('/content/stat_lm.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600bcef7",
      "metadata": {
        "id": "600bcef7"
      },
      "source": [
        "Тут мы для токенизатора сохраняем только спецтокены и словарь, для модели - параметры и статистики n-грамм и n+1-грамм. Потом в телеграм боте подгружаем именно эти параметры"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dbf63a9",
      "metadata": {
        "id": "1dbf63a9"
      },
      "source": [
        "Когда обучите модель на большом датасете, советую посмотреть на распределение вероятностей для следующего слова при разных входах"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e659ff6",
      "metadata": {
        "id": "0e659ff6"
      },
      "source": [
        "### смотрим как конструировать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0ad87959",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "0ad87959",
        "outputId": "ca29af45-45bd-4a4b-8f67-dbdfbf412422"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'models/stat_lm/tokenizer.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2e01aa11d76b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"дошик\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5411c4ee8794>\u001b[0m in \u001b[0;36mconstruct_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstat_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStatLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c457bc112c64>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/stat_lm/tokenizer.pkl'"
          ]
        }
      ],
      "source": [
        "model, kwargs = construct_model()\n",
        "\n",
        "model.generate(\"дошик\", **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "517748d6",
      "metadata": {
        "id": "517748d6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d51863cc05c647159cef8e934b4df19d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2dff33985044707874ea8bce01207d5",
              "IPY_MODEL_66913ce89ee84cbe830052c6ce5d3d27",
              "IPY_MODEL_77d60cf04d5248a8ba8adbb52d32e303"
            ],
            "layout": "IPY_MODEL_31eba6d03bf742c7a554ba0f3c3396d4"
          }
        },
        "a2dff33985044707874ea8bce01207d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_185880a338bb4accb424a4b612566271",
            "placeholder": "​",
            "style": "IPY_MODEL_c27fe85b9d0344998143016ed0141db5",
            "value": "train lines:   0%"
          }
        },
        "66913ce89ee84cbe830052c6ce5d3d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b6ede97630f4bd698d5b132085a04e7",
            "max": 150552,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d4737b8d7124fe79af7248703656021",
            "value": 1
          }
        },
        "77d60cf04d5248a8ba8adbb52d32e303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5a36e574875457c818a6852208109cb",
            "placeholder": "​",
            "style": "IPY_MODEL_ebb58cf9ef2b4ba396cf05d9d5a1e3b1",
            "value": " 1/150552 [00:00&lt;1:56:37, 21.51it/s]"
          }
        },
        "31eba6d03bf742c7a554ba0f3c3396d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185880a338bb4accb424a4b612566271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27fe85b9d0344998143016ed0141db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b6ede97630f4bd698d5b132085a04e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4737b8d7124fe79af7248703656021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5a36e574875457c818a6852208109cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebb58cf9ef2b4ba396cf05d9d5a1e3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}